{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/selectwait/colab/blob/main/fuse_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c5341c93-8936-4f4e-8c72-a6014e3712ec",
      "metadata": {
        "id": "c5341c93-8936-4f4e-8c72-a6014e3712ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "cf9570f4-6b99-4505-eea3-7990c5549239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1EMCfy_FWfkpICZ7j6oeINsiRS7jAUoqe\n",
            "From (redirected): https://drive.google.com/uc?id=1EMCfy_FWfkpICZ7j6oeINsiRS7jAUoqe&confirm=t&uuid=ff97a624-0a66-466e-8e7e-1a4fa0380d9b\n",
            "To: /content/tensor.npz\n",
            "100%|██████████| 521M/521M [00:03<00:00, 165MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight tensor: (128, 2880, 90, 16), scale tensor: (128, 2880, 90)\n"
          ]
        }
      ],
      "source": [
        "#@title Import and Load GPT-OSS one MLP Tensor\n",
        "import gdown\n",
        "import numpy as np\n",
        "\n",
        "MXFP4_TENSOR_LINK = 'https://drive.google.com/uc?id=1EMCfy_FWfkpICZ7j6oeINsiRS7jAUoqe'\n",
        "tensor_npz = 'tensor.npz'\n",
        "gdown.download(MXFP4_TENSOR_LINK, tensor_npz, quiet=False)\n",
        "mlp_tensors = np.load(tensor_npz)\n",
        "mlp_weight, mlp_scale = mlp_tensors['arr_0'], mlp_tensors['arr_1']\n",
        "print(f'weight tensor: {mlp_weight.shape}, scale tensor: {mlp_scale.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2350b860-1ee9-4e97-bd57-2b7d9d0f79f7",
      "metadata": {
        "id": "2350b860-1ee9-4e97-bd57-2b7d9d0f79f7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Dequantization Tensor\n",
        "def make_fp4_e2m1_lut() -> np.ndarray:\n",
        "  \"\"\"Make lookup table for fp4 e2m1.\"\"\"\n",
        "  lut = np.zeros(16, dtype=np.float32)\n",
        "  for code in range(16):\n",
        "    s = (code >> 3) & 0x1\n",
        "    E = (code >> 1) & 0x3\n",
        "    M = code & 0x1\n",
        "    bias = 1\n",
        "    if E == 0:\n",
        "      val = (M / 2.0)  # subnormal\n",
        "    else:\n",
        "      frac = 1.0 + (M / 2.0)\n",
        "      exp  = E - bias\n",
        "      val  = np.ldexp(frac, exp)\n",
        "    lut[code] = (-1.0)**s * val\n",
        "  return lut\n",
        "\n",
        "# A Global value :-/\n",
        "_FP4_LUT = make_fp4_e2m1_lut()\n",
        "\n",
        "def e8m0_decode(scales_u8: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"Scale u8 dequantize E8M0.\"\"\"\n",
        "  return np.exp2(scales_u8.astype(np.int16) - 127)\n",
        "\n",
        "\n",
        "def mxfp4_dequantize(packed_fp4: np.ndarray,\n",
        "                     scales_u8: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"Dequantize MXFP4 tensor with scale.\n",
        "\n",
        "  Args:\n",
        "    packed_fp4: Packed MXFP4 tensor. np.ndarray(uint8), shape (..., B)\n",
        "                ach byte holds 2 FP4 values (low nibble first).\n",
        "    scales_u8: np.ndarray(uint8), shape (...) — same as packed_fp4.shape[:-1]\n",
        "               One E8M0 scale per block of 2*B elements.\n",
        "\n",
        "  Returns:\n",
        "    np.ndarray(float32), shape (..., 2*B)\n",
        "  \"\"\"\n",
        "  assert packed_fp4.dtype == np.uint8\n",
        "  assert scales_u8.dtype == np.uint8\n",
        "  assert packed_fp4.shape[:-1] == scales_u8.shape, \\\n",
        "      f\"scales shape {scales_u8.shape} must match packed_fp4.shape[:-1] {packed_fp4.shape[:-1]}\"\n",
        "\n",
        "  # unpack nibbles → (..., 2*B)\n",
        "  low  = packed_fp4 & 0x0F\n",
        "  high = packed_fp4 >> 4\n",
        "  nibbles = np.concatenate([low, high], axis=-1)\n",
        "\n",
        "  # FP4 decode via LUT\n",
        "  elems = _FP4_LUT[nibbles]\n",
        "\n",
        "  # decode scales and broadcast\n",
        "  scales = e8m0_decode(scales_u8)[..., None]  # expand last dim\n",
        "  return elems * scales\n",
        "\n",
        "\n",
        "def mxfp4_mlp_matmul_activation(\n",
        "    x: np.ndarray,\n",
        "    weight_packed: np.ndarray,\n",
        "    scale_u8: np.ndarray,\n",
        "    expert_idx: int,\n",
        "    bias: np.ndarray | None = None\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Fused Quantizatze and Matmul.\n",
        "\n",
        "    Args:\n",
        "      x: (..., intermediate) float32/float16 — activation vector(s) entering this MLP linear\n",
        "      weight_packed: Packed weight tensor (n_experts, intermediate, n_blocks, b) uint8\n",
        "      scale_u8: Scale tensor (n_experts, intermediate, n_blocks) uint8\n",
        "      expert_idx:   which expert to use.\n",
        "      bias: Optional bias tensor.\n",
        "\n",
        "    Returns:\n",
        "      (..., d_model) float32 — y = x @ W^T (+ bias)\n",
        "    \"\"\"\n",
        "    assert weight_packed.dtype == np.uint8 and scale_u8.dtype == np.uint8\n",
        "    assert weight_packed.shape[:-1] == scale_u8.shape\n",
        "    assert 0 <= expert_idx < weight_packed.shape[0]\n",
        "\n",
        "    intermediate = weight_packed.shape[1]\n",
        "    x = np.asarray(x)\n",
        "    assert x.shape[-1] == intermediate, f\"expected last dim {intermediate}, got {x.shape[-1]}\"\n",
        "    x = x.astype(np.float32, copy=False)\n",
        "\n",
        "    # output buffer (..., O)\n",
        "    out_shape = x.shape[:-1] + (weight_packed.shape[1],)  # (..., 2880)\n",
        "    y = np.zeros(out_shape, dtype=np.float32)\n",
        "\n",
        "    # grab expert slice once\n",
        "    Wp_e = weight_packed[expert_idx]     # For GPT-OSS (i, 90, 16)\n",
        "    Sc_e = scale_u8[expert_idx]          # For GPT-OSS (i, 90)\n",
        "\n",
        "    # process 32-wide input blocks\n",
        "    # For each block j: dequantize (O,32) then y += einsum('...k,ok->...o', x_block, W_block)\n",
        "    for j in range(Wp_e.shape[1]):\n",
        "        x_block = x[..., (j*32):((j+1)*32)]                      # (..., 32)\n",
        "        W_block = mxfp4_dequantize(Wp_e[:, j, :],      # (O,16) → (O,32)\n",
        "                                   Sc_e[:, j])         # (O,)\n",
        "        # Accumulate: batch-friendly\n",
        "        y += np.einsum('...k,ok->...o', x_block, W_block, optimize=True)\n",
        "\n",
        "    if bias is not None:\n",
        "        bias = np.asarray(bias, dtype=np.float32)\n",
        "        assert bias.shape == (weight_packed.shape[1],)\n",
        "        y += bias\n",
        "\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "092cc656-6d20-4709-a129-4bbe23fa9582",
      "metadata": {
        "id": "092cc656-6d20-4709-a129-4bbe23fa9582"
      },
      "outputs": [],
      "source": [
        "# Create a random activation and pick 4 random expert.\n",
        "dummy_activations = np.random.randn(10, 2880).astype(np.float32)\n",
        "experts = np.random.randint(0, 128, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5f907487-6c02-4861-8142-907e951884ca",
      "metadata": {
        "id": "5f907487-6c02-4861-8142-907e951884ca",
        "outputId": "8bd3ec26-08d0-4a05-de26-9297244849ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output shape: (10, 2880)\n",
            "[-59.60875702  69.62116623  -5.2908659   82.61049032  -7.77480698]\n",
            "CPU times: user 1.07 s, sys: 0 ns, total: 1.07 s\n",
            "Wall time: 545 ms\n"
          ]
        }
      ],
      "source": [
        "#@title Fused Performance\n",
        "%%time\n",
        "\n",
        "# --- Perform the fused multiplication ---\n",
        "output = np.zeros((10, 2880))\n",
        "for expert in experts:\n",
        "  output += mxfp4_mlp_matmul_activation(dummy_activations, mlp_weight, mlp_scale, expert)\n",
        "\n",
        "print(f\"\\nOutput shape: {output.shape}\")\n",
        "print(output[0, :5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "109f5111-8574-4a78-a5d8-d08c2478cbae",
      "metadata": {
        "id": "109f5111-8574-4a78-a5d8-d08c2478cbae",
        "outputId": "82165160-1f20-4d44-994b-5b65b0080e56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output shape: (10, 2880)\n",
            "[-59.60876846  69.62115955  -5.29086876  82.61053276  -7.77484131]\n",
            "CPU times: user 6.51 s, sys: 2.88 s, total: 9.39 s\n",
            "Wall time: 9.42 s\n"
          ]
        }
      ],
      "source": [
        "#@title Dequant and Matmul Performance\n",
        "%%time\n",
        "mat = mxfp4_dequantize(mlp_weight, mlp_scale)\n",
        "out = np.zeros((10, 2880))\n",
        "for expert in experts:\n",
        "  expert_mat = mat[expert].reshape((2880, -1))\n",
        "  out += np.matmul(dummy_activations, expert_mat.T)\n",
        "print(f\"\\nOutput shape: {out.shape}\")\n",
        "print(out[0, :5])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6TBkE7PY1TS9"
      },
      "id": "6TBkE7PY1TS9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}